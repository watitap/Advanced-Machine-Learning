{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9abdb591-2e1a-4d5c-b652-99428f56d4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\poope\\AppData\\Local\\Temp\\ipykernel_17376\\3692707589.py:27: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\n",
      "No GPU was detected. Neural nets can be very slow without a GPU.\n"
     ]
    }
   ],
   "source": [
    "# Collapse-show\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.test.is_gpu_available():\n",
    "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"rnn\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
    "              \"accelerator.\")\n",
    "    if \"kaggle_secrets\" in sys.modules:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96e5f8b-f509-4c93-860e-10be175f6630",
   "metadata": {},
   "source": [
    "## An Encoder–Decoder Network for Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffe702f-1772-49c9-a8f0-5dc9c7d6911c",
   "metadata": {},
   "source": [
    "### Load and Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b00b49-5441-435c-9ba7-2d2a26df92d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_text</th>\n",
       "      <th>th_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The fool wanders, the wise man travels.</td>\n",
       "      <td>คนโง่พเนจร คนฉลาดท่องเที่ยว</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One of these days is none of these days.</td>\n",
       "      <td>หนึ่งในวันเหล่านี้คือไม่มีวันเหล่านี้เลย</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Necessity is a hard nurse, but she raises stro...</td>\n",
       "      <td>ความจำเป็นเป็นสิ่งที่ยาก, แต่เธอสามารถเลี้ยงลู...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In one ear and out the other.</td>\n",
       "      <td>เข้าหู้ข้างหนึ่งและออกอีกข้างหนึ่ง</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It can't happen here is number one on the list...</td>\n",
       "      <td>ไม่สามารถเกิดขึ้นที่นี่ได้คือคำพูดสุดท้ายที่ดั...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Facts do not cease to exist because they are i...</td>\n",
       "      <td>ความจริงไม่ได้หายไปเพราะถูกเมิน</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pride cometh before a fall.</td>\n",
       "      <td>ความภาคภูมิมาก่อนที่จะล้ม</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>There is no fool like an old fool.</td>\n",
       "      <td>ไม่มีคนโง่อย่างเช่นคนแก่</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Use not today what tomorrow will need.</td>\n",
       "      <td>อย่าใช้สิ่งวันนี้ถ้าต้องการวันพรุ่งนี้</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>See nothing, say nothing, know nothing.</td>\n",
       "      <td>ไม่มอง ไม่พูด ไม่รู้</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en_text  \\\n",
       "0            The fool wanders, the wise man travels.   \n",
       "1           One of these days is none of these days.   \n",
       "2  Necessity is a hard nurse, but she raises stro...   \n",
       "3                      In one ear and out the other.   \n",
       "4  It can't happen here is number one on the list...   \n",
       "5  Facts do not cease to exist because they are i...   \n",
       "6                        Pride cometh before a fall.   \n",
       "7                 There is no fool like an old fool.   \n",
       "8             Use not today what tomorrow will need.   \n",
       "9            See nothing, say nothing, know nothing.   \n",
       "\n",
       "                                             th_text  \n",
       "0                        คนโง่พเนจร คนฉลาดท่องเที่ยว  \n",
       "1           หนึ่งในวันเหล่านี้คือไม่มีวันเหล่านี้เลย  \n",
       "2  ความจำเป็นเป็นสิ่งที่ยาก, แต่เธอสามารถเลี้ยงลู...  \n",
       "3                 เข้าหู้ข้างหนึ่งและออกอีกข้างหนึ่ง  \n",
       "4  ไม่สามารถเกิดขึ้นที่นี่ได้คือคำพูดสุดท้ายที่ดั...  \n",
       "5                    ความจริงไม่ได้หายไปเพราะถูกเมิน  \n",
       "6                          ความภาคภูมิมาก่อนที่จะล้ม  \n",
       "7                           ไม่มีคนโง่อย่างเช่นคนแก่  \n",
       "8             อย่าใช้สิ่งวันนี้ถ้าต้องการวันพรุ่งนี้  \n",
       "9                               ไม่มอง ไม่พูด ไม่รู้  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('mozilla_common_voice.csv')\n",
    "data.head(10) # display the first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dfda232-192b-41c8-b460-cd54fd6a6f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                  en_text  \\\n",
       "0                The fool wanders, the wise man travels.   \n",
       "1               One of these days is none of these days.   \n",
       "2      Necessity is a hard nurse, but she raises stro...   \n",
       "3                          In one ear and out the other.   \n",
       "4      It can't happen here is number one on the list...   \n",
       "...                                                  ...   \n",
       "33792                   If you can't help, don't hinder.   \n",
       "33793                           It's all in a days work.   \n",
       "33794  Laziness travels so slowly that poverty soon o...   \n",
       "33795  Pushchairs can be folded when the toddler want...   \n",
       "33796  Quantum computing machines are rare and hard t...   \n",
       "\n",
       "                                                 th_text  \n",
       "0                            คนโง่พเนจร คนฉลาดท่องเที่ยว  \n",
       "1               หนึ่งในวันเหล่านี้คือไม่มีวันเหล่านี้เลย  \n",
       "2      ความจำเป็นเป็นสิ่งที่ยาก, แต่เธอสามารถเลี้ยงลู...  \n",
       "3                     เข้าหู้ข้างหนึ่งและออกอีกข้างหนึ่ง  \n",
       "4      ไม่สามารถเกิดขึ้นที่นี่ได้คือคำพูดสุดท้ายที่ดั...  \n",
       "...                                                  ...  \n",
       "33792                  หากคุณไม่สามารถช่วยได้อย่าขัดขวาง  \n",
       "33793                                       ทำงานทั้งวัน  \n",
       "33794  ความเกียจคร้านเดินทางช้ามากจนความยากจนมาถึงในไ...  \n",
       "33795      สามารถพับเก็บเก้าอี้ลงได้เมื่อทารกต้องการเดิน  \n",
       "33796      เครื่องคำนวณควอนตัมทั้งนั้นหายากและผลิตได้ยาก  \n",
       "\n",
       "[33797 rows x 2 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info # summary of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d774c4ce-723c-4b60-b388-c17416bb2e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en_text    0\n",
       "th_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum() # Check if there is null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33447ba9-ccf4-4486-b603-2a20fc8eb925",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/poope/env/MLII_asg/mozilla_common_voice.csv', encoding='utf-8')\n",
    "\n",
    "# Extract only the desired columns\n",
    "desired_columns = [\"en_text\", \"th_text\"]\n",
    "df_subset = df[desired_columns]\n",
    "\n",
    "# Write the subset to a new text file with tab separation\n",
    "text_file_path = 'C:/Users/poope/env/MLII_asg/output.txt'\n",
    "newData = df_subset.to_csv(text_file_path, sep='\\t', index=False, header=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b674c870-2607-49f4-840e-90fea772cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the file\n",
    "with open('C:/Users/poope/env/MLII_asg/output.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e21d867-b0df-4e51-b414-3a1b2c659dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "text = text.replace(\"!\", \"\").replace(\".\", \"\")\n",
    "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "np.random.seed(42)  # extra code – ensures reproducibility on CPU\n",
    "np.random.shuffle(pairs)\n",
    "sentences_en, sentences_th = zip(*pairs)  # separates the pairs into 2 lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ef80fc6-6a93-4d55-b6dd-53d6c8969e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You see, a minute goes by so fearfully quick => คุณเห็นแล้วใช่ไหมว่าแต่ละนาทีผ่านไปอย่างรวดเร็ว\n",
      "Such is my passage engaged on the steamer => นี่คือข้อที่ฉันใช้บนเรือกลไฟ\n",
      "The Queen sampled the selection of cakes => พระราชินีลองชิมเค้กที่แบ่งเป็นชิ้นไว้\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(sentences_en[i], \"=>\", sentences_th[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1a70b02-7ab6-4fec-8c50-82180567aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "max_length = 500\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_th = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_th.adapt([f\"startofseq {s} endofseq\" for s in sentences_th])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04e3cb62-edf5-45b4-a2c0-01f3557c28cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be', 'this', 'her', 'are', 'his', 'with', 'on', 'we', 'what', 'said']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_en.get_vocabulary()[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa6d7ce4-3f13-46e7-8aff-6c207a6135f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5',\n",
       " '3',\n",
       " 'ฉันกล่าว',\n",
       " 'เบอร์ตี้',\n",
       " 'ฉันพูด',\n",
       " 'คือ',\n",
       " 'กล่าว',\n",
       " 'มา',\n",
       " 'คุณผู้หญิง',\n",
       " 'adele']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_th.get_vocabulary()[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2189f733-e15e-43de-8eb8-3c5731dd45f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_en.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "713848f1-3b5f-4d9b-989d-1c116840432a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_string_items = []\n",
    "for idx, val in enumerate(sentences_th):\n",
    "    if not isinstance(val, str):\n",
    "        non_string_items.append((idx, val))\n",
    "non_string_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "093614bc-efb3-441c-8f0f-1055bcbfa8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset processed successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('คุณเห็นแล้วใช่ไหมว่าแต่ละนาทีผ่านไปอย่างรวดเร็ว',\n",
       " 'นี่คือข้อที่ฉันใช้บนเรือกลไฟ',\n",
       " 'พระราชินีลองชิมเค้กที่แบ่งเป็นชิ้นไว้',\n",
       " 'มีดยาวปักทะลุหัวใจของเขาจนทำให้เขาต้องล้มลงไปกับพื้น',\n",
       " 'มันอาจจะเป็นสิ่งแรกที่พวกเขาคิดถึง',\n",
       " 'แล้วเกิดอะไรขึ้นกับเขา',\n",
       " 'เอสเธอร์เองก็คิดเช่นนั้น',\n",
       " 'เศษที่ขาดไปของจดหมาย หมายถึงอะไร?',\n",
       " 'ที่อยู่นี้ราคาเอื้อมถึงและสะดวกสบาย',\n",
       " 'วัสดุอินทรีย์จะสลายตัวได้ตามธรรมชาติซึ่งแตกต่างจากพลาสติกอนินทรีย์')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a subset\n",
    "subset = sentences_th[:10]  # Starting with the first 10 elements\n",
    "\n",
    "# Try processing the subset\n",
    "try:\n",
    "    processed_subset = text_vec_layer_th(subset)\n",
    "    print(\"Subset processed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error encountered: {e}\")\n",
    "\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e29f056e-1b38-4db1-8dbc-709a9703f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.constant(sentences_en[:33_000])\n",
    "X_valid = tf.constant(sentences_en[33_000:])\n",
    "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_th[:33_000]])\n",
    "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_th[33_000:]])\n",
    "Y_train = text_vec_layer_th([f\"{s} endofseq\" for s in sentences_th[:33_000]])\n",
    "Y_valid = text_vec_layer_th([f\"{s} endofseq\" for s in sentences_th[33_000:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03990c7d-6797-4ea6-a813-0afc10686d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d542331-8f0e-4e7d-ae18-75e9ee4e10a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_th(decoder_inputs)\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8436c66a-ea7e-4bf4-8338-62d9d1d847a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tf.keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb4c6ce5-4704-4158-8d2c-7a5dd6af0116",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eb3fbeb-f319-4fb8-b943-773f3835d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272ab21-387f-4222-bdae-d0550f726877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "129/129 [==============================] - 51461s 402s/step - loss: 1.2277 - accuracy: 0.8186 - val_loss: 0.8148 - val_accuracy: 0.8371\n",
      "Epoch 2/10\n",
      "129/129 [==============================] - 26048s 202s/step - loss: 0.8234 - accuracy: 0.8444 - val_loss: 0.8004 - val_accuracy: 0.8398\n",
      "Epoch 3/10\n",
      " 57/129 [============>.................] - ETA: 1:24:55 - loss: 0.7954 - accuracy: 0.8476"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Regularize validation set and stop once the model creases to improve.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid), batch_size=256, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6bbf08-b20c-4fce-b426-dca3cf8f4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence_en):\n",
    "    translation = \"\"\n",
    "    for word_idx in range(max_length):\n",
    "        X = np.array([sentence_en])  # encoder input \n",
    "        X_dec = np.array([\"startofseq \" + translation])  # decoder input\n",
    "        y_proba = model.predict((X, X_dec))[0, word_idx]  # last token's probas\n",
    "        predicted_word_id = np.argmax(y_proba)\n",
    "        predicted_word = text_vec_layer_th.get_vocabulary()[predicted_word_id]\n",
    "        if predicted_word == \"endofseq\":\n",
    "            break\n",
    "        translation += \" \" + predicted_word\n",
    "    return translation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e344508-445c-4c54-b496-dbeb8db10e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('Facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d901e926-5798-431c-a288-3857762092fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('Sound as a bell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b6097e-efaf-4155-9517-782c4edba129",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "tf.test.is_built_with_cuda()\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d784260-9e7a-4cfa-bbef-c5df23b4af8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579f18bc-e65f-49ab-bbb5-86bafdbeb60d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
